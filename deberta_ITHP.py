from torch import nn
from transformers.models.deberta_v2.modeling_deberta_v2 import DebertaV2PreTrainedModel, DebertaV2Model
from transformers.models.bert.modeling_bert import BertPooler
from ITHP import ITHP
import global_configs
from global_configs import DEVICE


class ITHP_DebertaModel(DebertaV2PreTrainedModel):
    def __init__(self, config, multimodal_config):
        super().__init__(config)
        TEXT_DIM, ACOUSTIC_DIM, VISUAL_DIM = (
            global_configs.TEXT_DIM, global_configs.ACOUSTIC_DIM, global_configs.VISUAL_DIM
        )

        self.pooler = BertPooler(config)
        self.model = DebertaV2Model.from_pretrained("microsoft/deberta-v3-base").to(DEVICE)

        ITHP_args = {
            'X0_dim': TEXT_DIM,
            'X1_dim': ACOUSTIC_DIM,
            'X2_dim': VISUAL_DIM,
            'B0_dim': multimodal_config.B0_dim,
            'B1_dim': multimodal_config.B1_dim,
            'inter_dim': multimodal_config.inter_dim,
            'max_sen_len': multimodal_config.max_seq_length,
            'drop_prob': multimodal_config.drop_prob,
            'p_beta': multimodal_config.p_beta,
            'p_gamma': multimodal_config.p_gamma,
            'p_lambda': multimodal_config.p_lambda,
            # ğŸ”¥ æ·»åŠ é—¨æ§æ¨¡å¼é…ç½®
            'gating_mode': getattr(multimodal_config, 'gating_mode', 'dual_gating'),
        }

        self.ITHP = ITHP(ITHP_args)
        self.expand = nn.Linear(multimodal_config.B1_dim, TEXT_DIM)
        
        # ğŸ”¥ ä¸ºå£°å­¦å’Œè§†è§‰é‡æ„ç‰¹å¾æ·»åŠ æŠ•å½±å±‚
        self.acoustic_proj = nn.Linear(ACOUSTIC_DIM, TEXT_DIM)
        self.visual_proj = nn.Linear(VISUAL_DIM, TEXT_DIM)
        
        self.LayerNorm = nn.LayerNorm(config.hidden_size)
        self.dropout = nn.Dropout(multimodal_config.dropout_prob)
        self.beta_shift = multimodal_config.beta_shift

        # ğŸ”¥ æ·»åŠ æ¶ˆèå®éªŒæ§åˆ¶å‚æ•°
        self.fusion_mode = getattr(multimodal_config, 'fusion_mode', 'full')
        # fusion_mode å¯é€‰å€¼:
        # 'b1_only': åªä½¿ç”¨B1 (æ–‡æœ¬å‹ç¼©ä¿¡æ¯)
        # 'b1_acoustic': B1 + å£°å­¦é‡æ„
        # 'b1_visual': B1 + è§†è§‰é‡æ„  
        # 'full': B1 + å£°å­¦é‡æ„ + è§†è§‰é‡æ„ (é»˜è®¤)

        # ğŸ”¥ æ·»åŠ é—¨æ§æ¨¡å¼é…ç½®
        self.gating_mode = getattr(multimodal_config, 'gating_mode', 'dual_gating')
        # gating_mode å¯é€‰å€¼:
        # 'no_gating': ç§»é™¤æ‰€æœ‰é—¨æ§
        # 'single_gating': ä»…ç¬¬ä¸€å±‚ä½¿ç”¨é—¨æ§
        # 'dual_gating': ä¸¤å±‚éƒ½ä½¿ç”¨é—¨æ§ (é»˜è®¤)

        self.init_weights()

    def forward(self, input_ids, visual, acoustic, attention_mask=None, epoch=0, max_epochs=40):
        embedding_output = self.model(input_ids, attention_mask=attention_mask)
        x = embedding_output[0]  # token-levelè¡¨å¾

        b1, IB_total, kl_loss_0, mse_0, kl_loss_1, mse_1, intermediate_results = self.ITHP(
            x, acoustic, visual, epoch, max_epochs
        )
        
        # è·å–é‡æ„ç»“æœ
        reconstructions = intermediate_results['reconstructions']
        
        # æ‰©å±•B1åˆ°TEXT_DIM
        h_m = self.expand(reconstructions['b1'])  # ä½¿ç”¨çº¯B1ç‰¹å¾
        
        # ğŸ”¥ æ ¹æ®æ¶ˆèæ¨¡å¼é€‰æ‹©èåˆç­–ç•¥
        if self.fusion_mode == 'b1_only':
            # æ–¹æ¡ˆ1: åªä½¿ç”¨B1 (æ–‡æœ¬å‹ç¼©ä¿¡æ¯)
            acoustic_vis_embedding = self.beta_shift * h_m
            
        elif self.fusion_mode == 'b1_acoustic':
            # æ–¹æ¡ˆ2: B1 + å£°å­¦é‡æ„
            acoustic_recon = reconstructions['acoustic_recon']  # [batch, seq_len, acoustic_dim]
            acoustic_proj = self.acoustic_proj(acoustic_recon)  # æŠ•å½±åˆ°TEXT_DIM
            acoustic_vis_embedding = self.beta_shift * (h_m + acoustic_proj)
            
        elif self.fusion_mode == 'b1_visual':
            # æ–¹æ¡ˆ3: B1 + è§†è§‰é‡æ„
            visual_recon = reconstructions['visual_recon']  # [batch, seq_len, visual_dim]
            visual_proj = self.visual_proj(visual_recon)  # æŠ•å½±åˆ°TEXT_DIM
            acoustic_vis_embedding = self.beta_shift * (h_m + visual_proj)
            
        elif self.fusion_mode == 'full':
            # æ–¹æ¡ˆ4: B1 + å£°å­¦é‡æ„ + è§†è§‰é‡æ„ (å®Œæ•´æ¨¡å‹)
            acoustic_recon = reconstructions['acoustic_recon']
            visual_recon = reconstructions['visual_recon']
            acoustic_proj = self.acoustic_proj(acoustic_recon)
            visual_proj = self.visual_proj(visual_recon)
            acoustic_vis_embedding = self.beta_shift * (h_m + acoustic_proj + visual_proj)
        
        else:
            raise ValueError(f"Unknown fusion_mode: {self.fusion_mode}")

        sequence_output = self.dropout(self.LayerNorm(acoustic_vis_embedding + x))
        pooled_output = self.pooler(sequence_output)

        return pooled_output, IB_total, kl_loss_0, mse_0, kl_loss_1, mse_1


class ITHP_DeBertaForSequenceClassification(DebertaV2PreTrainedModel):
    def __init__(self, config, multimodal_config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.dberta = ITHP_DebertaModel(config, multimodal_config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

        self.init_weights()

    def forward(self, input_ids, visual, acoustic, attention_mask=None, epoch=0, max_epochs=40):
        pooled_output, IB_total, kl_loss_0, mse_0, kl_loss_1, mse_1 = self.dberta(
            input_ids, visual, acoustic, attention_mask=attention_mask, epoch=epoch, max_epochs=max_epochs
        )

        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        return logits, IB_total, kl_loss_0, mse_0, kl_loss_1, mse_1
